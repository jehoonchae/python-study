{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = './example_En_docs/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "total_docs = []\n",
    "for file in onlyfiles:\n",
    "    file_path = mypath+file\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        content = f.read()\n",
    "    total_docs.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_total_docs = [doc.replace('\\n', ' ') for doc in total_docs]\n",
    "DOCS_NUM = len(cleaned_total_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing # import a python file for English text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['be', 'today', 'yesterday', 'new', 'york', 'time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [preprocessing.En_processing(doc, stopwords) for doc in cleaned_total_docs] # (text, list of noun words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs_nouns = []\n",
    "docs_nouns = []\n",
    "for doc, words in docs:\n",
    "    total_docs_nouns.extend(words)\n",
    "    docs_nouns.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # frequency based DTM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # tf-idf based DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_extractor(corpus): \n",
    "    # returns a frequency-based DTM\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1))\n",
    "    # min_df: minimum document frequency threshold\n",
    "         # if min_df = 1 -> 최소 적어도 하나의 문서에서 사용된 단어들은 모두 포함\n",
    "    # ngram_range=(1,1) -> unigram 만 포함\n",
    "    # ngram_range=(1,3) -> unigram, bi-gram, tri-gram 까지 고려\n",
    "    features = vectorizer.fit_transform(corpus) # transform texts to a frequency matrix\n",
    "    return vectorizer, features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_extractor(corpus):\n",
    "    # returns a tf-idf based DTM\n",
    "    vectorizer = TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=(1,1))\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_filtered = []\n",
    "for doc in docs_nouns:\n",
    "    document_filtered =''\n",
    "    for word in doc:\n",
    "        document_filtered = document_filtered+' '+word\n",
    "    documents_filtered.append(document_filtered) # list of strings\n",
    "# 불필요한 단어들을 제거하고 난후 DTM로 변환하기 위해 다시 list of strings의 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, doc_term_mat_tfidf = tfidf_extractor(documents_filtered) # TFIDF matrix로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dense = doc_term_mat_tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 0.        , 0.04116293,\n",
       "         0.04116293],\n",
       "        [0.        , 0.02776663, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.04711425, 0.0409108 , 0.04711425, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM_TFIDF = np.array(tfidf_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.04116293,\n",
       "        0.04116293],\n",
       "       [0.        , 0.02776663, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.04711425, 0.0409108 , 0.04711425, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AgglomerativeClustering이라는 class가 위계적 군집화를 위한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 경우에는 single 방법(즉, 가장 거리가 가까운 벡터 간의 거리를 구해서 비교하는 방법)을 이용한다. 거리를 계산한 방식은 코사인 유사도이고, 미리 연구자가 정해주는 군집의 수는 5개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_idf = AgglomerativeClustering(linkage='single', affinity='cosine', n_clusters=5).fit(DTM_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2, 1, 1, 4, 4, 0, 0, 4, 0, 3, 3, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_idf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_idf1 = AgglomerativeClustering(linkage='single', affinity='cosine', n_clusters=4).fit(tfidf_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 0, 0, 2, 2, 0, 2, 3, 3, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_idf1.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.3'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04510008]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf_dense[0], tfidf_dense[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04510008]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(DTM_TFIDF[0].reshape(1,-1), DTM_TFIDF[1].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40498389]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(DTM_TFIDF[4].reshape(1,-1), DTM_TFIDF[5].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity # cosine similarity 계산하기\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(list(range(DOCS_NUM)))\n",
    "for pair in list(itertools.combinations(range(DOCS_NUM), 2)):  # 15개의 문서에서 임의로 2개의 문서를 정하는 것이다.\n",
    "    similarity = cosine_similarity(DTM_TFIDF[pair[0]].reshape(1,-1), DTM_TFIDF[pair[1]].reshape(1,-1))[0][0]\n",
    "    # 코사인 유사도를 구해보고, 아래의 경우처럼 코사인 유사도의 값이 0.2보다 큰 경우에만 네트워크를 형성시킴으로써 군집화를 시행할 수 있다.\n",
    "    # 우리는 경우에 따라서 아래의 역치를 올리거나 내릴 수 있다.\n",
    "    if similarity>0.2: # 문서간의 유사도 기준을 0.35로 한 경우\n",
    "        g.add_edge(pair[0], pair[1], weight=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jehoonchae/opt/anaconda3/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if not cb.iterable(width):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "nx.draw_networkx(g)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 결과를 시각화했는데, 과연 어떠한 문서들 간에 군집화가 이루어진 것인지 역으로 다시 찾아야한다. 우리는 위에 있는 5개의 군집 각각을 component라고 일컫는데, 아래에서 nx.connected_components()를 이용하면 찾아낼 수 있게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0, 2, 4, 5}, {1, 3}, {6, 7, 10}, {8, 9, 11}, {12, 13, 14}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nx.connected_components(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
